{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Austin Caudill\n",
    "11/04/2021\n",
    "\n",
    "Submission for Avery Smith's Data Science Hackathon\n",
    "\n",
    "Questions to be answered:\n",
    "What affects open %?​\n",
    "    Send time, day of week, word count, link count ​\n",
    "Link sheets No-SQL database​\n",
    "What link is the most popular?​\n",
    "What topic is most popular?​\n",
    "What drives link clicks? ​\n",
    "How do ads affect?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "from autoviz.AutoViz_Class import AutoViz_Class\n",
    "import urllib3\n",
    "import certifi\n",
    "import validators\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    # Legacy Python that doesn't verify HTTPS certificates by default\n",
    "    pass\n",
    "else:\n",
    "    # Handle target environment that doesn't support HTTPS verification\n",
    "    ssl._create_default_https_context = _create_unverified_https_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "email_summary = pd.read_excel(r'summary.xlsx', parse_dates=[0])\n",
    "email_summary['Date'] = email_summary['Date/Time'].dt.date\n",
    "email_summary['Time'] = email_summary['Date/Time'].dt.time\n",
    "email_summary['Hour'] = email_summary['Date/Time'].dt.hour\n",
    "email_summary['Day'] = email_summary['Date/Time'].dt.day_name()\n",
    "\n",
    "files = os.listdir('link_data')\n",
    "link_data = pd.DataFrame() # Initialize dataframe\n",
    "for f in files:\n",
    "    data = pd.read_excel('./link_data/'+f, 'Sheet1')\n",
    "    link_data = link_data.append(data)\n",
    "\n",
    "# Need to cleanup by removing rows with \"nan\"\n",
    "link_data = link_data.dropna()\n",
    "\n",
    "result = []\n",
    "# Need to remove bad URLs\n",
    "for l in link_data['Link']:\n",
    "    try:\n",
    "        test = validators.url(l)\n",
    "        result.append(test)\n",
    "    except:\n",
    "        result.append(\"FAILED\")\n",
    "\n",
    "link_data['Result'] = result\n",
    "cleaned_URLs = link_data.loc[link_data['Result'] == True]\n",
    "# Remove filetypes that cannot be scraped.\n",
    "cleaned_URLs = cleaned_URLs[~cleaned_URLs.Link.str.contains('pdf|jpg|jpeg|JPG|png|cgi|creativecommons')]\n",
    "\n",
    "# Combine duplicates\n",
    "cleaned_URLs = cleaned_URLs.groupby(by='Link', as_index=False)[['Clicks']].sum()\n",
    "\n",
    "threshold = 80 # Minimum number of clicks before a link is evaluated.\n",
    "cleaned_URLs = cleaned_URLs.loc[cleaned_URLs['Clicks'] > threshold]\n",
    "\n",
    "\n",
    "# Perform EDA\n",
    "AV = AutoViz_Class()\n",
    "filename = \"\" # Not Needed\n",
    "dft = AV.AutoViz(\n",
    "    filename,\n",
    "    sep=\",\",\n",
    "    depVar=\"Open Rate\", # Target Variable\n",
    "    dfte=email_summary,\n",
    "    header=0,\n",
    "    verbose=1,\n",
    "    lowess=False,\n",
    "    chart_format=\"svg\",\n",
    ")\n",
    "\n",
    "\n",
    "# Time to scrape link data\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "http = urllib3.PoolManager(cert_reqs='CERT_REQUIRED',ca_certs=certifi.where())\n",
    "soupey = []\n",
    "for link in cleaned_URLs['Link']:\n",
    "    try:\n",
    "        print(\"Getting:\", link)\n",
    "        req = http.request('GET', link, headers=headers, retries=urllib3.Retry(redirect=2, raise_on_redirect=False))\n",
    "        if req.status != 200: \n",
    "            continue\n",
    "        soup = BeautifulSoup(req.data, \"html.parser\")\n",
    "        body = soup.find('body')\n",
    "        text = body.get_text()\n",
    "        soupey.append(text)\n",
    "    except:\n",
    "        print('failed')\n",
    "        continue\n",
    "\n",
    "\n",
    "# Text Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words_ext = ['http','n', 'please', 'nthe', 'license', 'cc', 'nmore', 'xa', 'c', 'u', 'r', 'f', 'licensing','licensed', 'licenses', 'creative commons','used','copyright','ha','wa','edit', 'archived', 'original'] # custom word exclusion list.\n",
    "\n",
    "def preprocess(text):\n",
    "\n",
    "    new_tokens = word_tokenize(text)\n",
    "    new_tokens = [t for t in new_tokens if t.isalpha()]\n",
    "    new_tokens = [t.lower() for t in new_tokens]\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    new_tokens =[lemmatizer.lemmatize(t) for t in new_tokens]\n",
    "\n",
    "    cleaned_words = []\n",
    "    # remove stopwords\n",
    "    for word in new_tokens:\n",
    "        if word not in stop_words and word not in stop_words_ext:\n",
    "            cleaned_words.append(word)\n",
    "    \n",
    "    #counts the words, pairs and trigrams\n",
    "    counted = Counter(cleaned_words)\n",
    "    counted_2= Counter(ngrams(cleaned_words,2))\n",
    "    counted_3= Counter(ngrams(cleaned_words,3))\n",
    "    #creates 3 data frames and returns thems\n",
    "    word_freq = pd.DataFrame(counted.items(),columns=['word','frequency']).sort_values(by='frequency',ascending=False)\n",
    "    word_pairs =pd.DataFrame(counted_2.items(),columns=['pairs','frequency']).sort_values(by='frequency',ascending=False)\n",
    "    trigrams =pd.DataFrame(counted_3.items(),columns=['trigrams','frequency']).sort_values(by='frequency',ascending=False)\n",
    "\n",
    "    return cleaned_words,word_freq,word_pairs,trigrams\n",
    "\n",
    "\n",
    "soupey = str(soupey)\n",
    "cleaned_soupey,word_freq,word_pairs,trigrams = preprocess(soupey)    \n",
    "\n",
    "number_of_words = len(cleaned_soupey)\n",
    "\n",
    "\n",
    "cleaned_soupey = \" \".join(cleaned_soupey)\n",
    "\n",
    "\n",
    "mask = np.array(Image.open(\"flask3.png\"))\n",
    "wordcloud = WordCloud(background_color ='white', prefer_horizontal=1, mask=mask, contour_width=5, contour_color='black', colormap='bone').generate((cleaned_soupey))\n",
    "\n",
    "# plot the WordCloud image \n",
    "plt.figure( figsize=(20,10) )                       \n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create subplot of the different data frames\n",
    "fig, axes = plt.subplots(3,1,figsize=(8,20))\n",
    "sns.barplot(ax=axes[0],x='frequency',y='word',data=word_freq.head(30))\n",
    "sns.barplot(ax=axes[1],x='frequency',y='pairs',data=word_pairs.head(30))\n",
    "sns.barplot(ax=axes[2],x='frequency',y='trigrams',data=trigrams.head(30))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Script Finished\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "91206f48b6387f687cf5ae936ff1df4efe51838fdde0c1d72033a292bf38bc07"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('hackathon': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
